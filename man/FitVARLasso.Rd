% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/RcppExports.R
\name{FitVARLasso}
\alias{FitVARLasso}
\title{Fit Vector Autoregressive (VAR) Model Parameters
using Lasso Regularization}
\usage{
FitVARLasso(YStd, XStd, lambda, max_iter, tol)
}
\arguments{
\item{YStd}{Numeric matrix.
Matrix of standardized dependent variables (Y).}

\item{XStd}{Numeric matrix.
Matrix of standardized predictors (X).
\code{XStd} should not include a vector of ones in column one.}

\item{lambda}{Numeric.
Lasso hyperparameter.
The regularization strength controlling the sparsity.}

\item{max_iter}{Integer.
The maximum number of iterations
for the coordinate descent algorithm
(e.g., \code{max_iter = 10000}).}

\item{tol}{Numeric.
Convergence tolerance.
The algorithm stops when the change in coefficients
between iterations is below this tolerance
(e.g., \code{tol = 1e-5}).}
}
\value{
Matrix of estimated autoregressive
and cross-regression coefficients.
}
\description{
This function estimates the parameters of a VAR model
using the Lasso regularization method
with cyclical coordinate descent.
The Lasso method is used to estimate the autoregressive
and cross-regression coefficients with sparsity.
}
\details{
The \code{\link[=FitVARLasso]{FitVARLasso()}} function estimates the parameters
of a Vector Autoregressive (VAR) model
using the Lasso regularization method.
Given the input matrices \code{YStd} and \code{XStd},
where \code{YStd} is the matrix of standardized dependent variables,
and \code{XStd} is the matrix of standardized predictors,
the function computes the autoregressive and cross-regression coefficients
of the VAR model with sparsity induced by the Lasso regularization.

The steps involved in estimating the VAR model parameters
using Lasso are as follows:
\itemize{
\item \strong{Initialization}: The function initializes the coefficient matrix
\code{beta} with OLS estimates.
The \code{beta} matrix will store the estimated autoregressive and
cross-regression coefficients.
\item \strong{Coordinate Descent Loop}: The function performs
the cyclical coordinate descent algorithm
to estimate the coefficients iteratively.
The loop iterates \code{max_iter} times,
or until convergence is achieved.
The outer loop iterates over the predictor variables
(columns of \code{XStd}),
while the inner loop iterates over the outcome variables
(columns of \code{YStd}).
\item \strong{Coefficient Update}: For each predictor variable (column of \code{XStd}),
the function iteratively updates the corresponding column of \code{beta}
using the coordinate descent algorithm with L1 norm regularization
(Lasso).
The update involves calculating the soft-thresholded value \code{c},
which encourages sparsity in the coefficients.
The algorithm continues until the change in coefficients
between iterations is below the specified tolerance \code{tol}
or when the maximum number of iterations is reached.
\item \strong{Convergence Check}: The function checks for convergence
by comparing the current \code{beta}
matrix with the previous iteration's \code{beta_old}.
If the maximum absolute difference between \code{beta} and \code{beta_old}
is below the tolerance \code{tol},
the algorithm is considered converged, and the loop exits.
}
}
\examples{
YStd <- StdMat(dat_p2_yx$Y)
XStd <- StdMat(dat_p2_yx$X[, -1]) # remove the constant column
lambda <- 73.90722
FitVARLasso(
  YStd = YStd,
  XStd = XStd,
  lambda = lambda,
  max_iter = 10000,
  tol = 1e-5
)

}
\seealso{
Other Fitting Autoregressive Model Functions: 
\code{\link{FitVARLassoSearch}()},
\code{\link{FitVAROLS}()},
\code{\link{LambdaSeq}()},
\code{\link{OrigScale}()},
\code{\link{PBootVARExoLasso}()},
\code{\link{PBootVARExoOLS}()},
\code{\link{PBootVARLasso}()},
\code{\link{PBootVAROLS}()},
\code{\link{RBootVARExoLasso}()},
\code{\link{RBootVARExoOLS}()},
\code{\link{RBootVARLasso}()},
\code{\link{RBootVAROLS}()},
\code{\link{SearchVARLasso}()},
\code{\link{StdMat}()}
}
\author{
Ivan Jacob Agaloos Pesigan
}
\concept{Fitting Autoregressive Model Functions}
\keyword{fit}
\keyword{fitAutoReg}
